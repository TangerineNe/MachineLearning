<h1>Метрические алгоритмы классификации</h1>
Мы будем рассматривать такие алгоритмы классификации, как:
- 1NN - 1 Nearst Neighbor
- kNN - k Nearst Neighbor
- kWNN - k Weighted Nearest Neighbors
- Метод парзеновского окна
- Метод потенциальных функций
И вспомогательные алгоритмы и понятия для улучшения качества моделей:
- LOO - Leave One Out
- CV - Cross Validation
- Отступ
- STOLP
- FRiS-Функция
- FRiS-STOLP


Наши алгоритмы мы будем испытывать на датасете "Ирисы Фишера".
Наглядное изображение нашей выборки:
![screenshot of sample](https://github.com/TangerineNe/MachineLearning/blob/main/images/iris.PNG)
Для классификации будем использовать Евклидову метрику. При необходимости можно подобрать другую.

<h2>1NN - 1 Nearst Neighbor</h2>
Суть алгоритма заключается в поиске единственного ближайшего соседа, после чего классифицируемому объекту присваивается его класс.
Одним из его преимуществ является простота реализации, но выбрав его вы будете вынуждены терпеть неусточивость к погрешностям  и невозможность настроить параметры по выборке, в связи с их отсутствием. Но вы можете попытаться подобрать другую метрику в надежде улучшить результат:)
Данный алгоритм является частным случаем алгоритма kNN, при k = 1.

<h2>kNN - k Nearst Neighbor</h2>
В отличие от 1NN мы берем не единственного ближайшего соседа, а k ближайших соседей, после чего присваиваем классифицируемому объекту доминирующий класс среди них.
В близи границы классов этот алгоритм все так же неустойчив к погрешностям, но в целом, его качество значительно выше. Но возникает вопрос в оптимальном подборе параметра k.
При  k=1 мы получим 1NN, и, соответственно, неустойчивость к шуму, а при слишком большом модель обращается в константу. Таким образом, крайние значения k нежелательны. На практике оптимальное k подбирается по критерию скользящего контроля LOO. Так же, существуют и альтернативный вариант метода kNN: в каждом классе выбирается k ближайших к классифицируемому объекту соседей, и классифицируемый объект относится к тому классу, для которого среднее расстояние до k ближайших соседей минимально.

<h2>kWNN - k Weighted Nearest Neighbors</h2>
А теперь, давайте рассмотрим случай метода knn при k=6. Предположим, что нам необходимо классифицировать объект, и вблизи него есть 2 соседа первого класса и 3 соседа второго класса. Но, в отличие от второго класса, 2 элемента первого класса находятся практически вплотную к классифицируемому элементу, в то время, как 3 соседа второго класса находятся довольно далеко. Исходя из здравого смысла, нам необходимо было бы отности классифицируемый объект к первому классу, но knn поведет себя иначе.
Напрашивается вывод, что нам следовало бы как-то учитывать удаленность элементов выборки. Именно это и делает kWNN.
kWNN - k ближайших взвешенных соседей, где вес определяется исходя из удаленности элементов.
При kNN каждый элемент имел единичный вес. После чего классифицируемый элемент принимал тот класс, в котором набралась бОьшая сумма весов. Но ведь фактически, весовую функцию можно определить различными способами. Например q^i, или (k+1-i)/k. Где i - ранг элемента, его номер в отсортированном по удаленности списке. А q - любое число в интервале (0; 1).
Таким образом, вместо подсчета кол-ва элементов, мы будем считать сумму их весов, после чего, классифицируемый элемент отнесем к тому классу, в котором набралась наибольшая сумма.

<h2>Метод Парзеновского окна</h2>
В методе Парзеновского окна вес элемента выборки зависит не от его ранга, а от расстояния до классифицируемого объекта.
Определим весовую функцию для элемента x_i следующим образом: w_i(a,x_i) = K(p(a, x_i)/h). Где K - Ядровая функция, a -- классифицируемый объект, x_i -- обучающий элемент выборки, h - размер окна, p - метрика.
Теперь, давайте попорядку.
<h3>Ядровая функция</h3>
Неотрицательная вещественнозначная интегрируемая функция K называется ядром.
В большинстве случаев желательно, чтобы функция удовлетворяла ещё двум требованиям:
-Номинирование
 
-Симметрия
![screenshot of sample](https://github.com/TangerineNe/MachineLearning/blob/main/images/%D0%A1%D0%B8%D0%BC%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%8F.PNG)

Часто используемые ядровые функции:







